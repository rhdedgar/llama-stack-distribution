#!/usr/bin/env bash

set -exuo pipefail

# Configuration
WORK_DIR="/tmp/llama-stack-integration-tests"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source common test utilities
# shellcheck source=/dev/null
source "$SCRIPT_DIR/test_utils.sh"

# Get repository and version dynamically from Containerfile
# Look for git URL format: git+https://github.com/*/llama-stack.git@vVERSION or @VERSION
CONTAINERFILE="$SCRIPT_DIR/../distribution/Containerfile"
GIT_URL=$(grep -o 'git+https://github\.com/[^/]\+/llama-stack\.git@v\?[0-9.+a-z]\+' "$CONTAINERFILE")
if [ -z "$GIT_URL" ]; then
    echo "Error: Could not extract llama-stack git URL from Containerfile"
    exit 1
fi

# Extract repo URL (remove git+ prefix and @version suffix)
LLAMA_STACK_REPO=${GIT_URL#git+}
LLAMA_STACK_REPO=${LLAMA_STACK_REPO%%@*}
# Extract version (remove git+ prefix and everything before @, and optional v prefix)
LLAMA_STACK_VERSION=${GIT_URL##*@}
LLAMA_STACK_VERSION=${LLAMA_STACK_VERSION#v}
if [ -z "$LLAMA_STACK_VERSION" ]; then
    echo "Error: Could not extract llama-stack version from Containerfile"
    exit 1
fi

function clone_llama_stack() {
    # Clone the repository if it doesn't exist
    if [ ! -d "$WORK_DIR" ]; then
        git clone "$LLAMA_STACK_REPO" "$WORK_DIR"
    fi

    # Checkout the specific tag
    cd "$WORK_DIR"
    # fetch origin incase we didn't clone a fresh repo
    git fetch origin
    if [ "$LLAMA_STACK_VERSION" == "main" ]; then
        checkout_to="main"
    else
        checkout_to="v$LLAMA_STACK_VERSION"
    fi
    if ! git checkout "$checkout_to"; then
        echo "Error: Could not checkout $checkout_to"
        echo "Available tags:"
        git tag | tail -10
        exit 1
    fi
}

function run_integration_tests() {
    validate_model_parameter "$1"
    local model="$1"
    echo "Running integration tests for model $model..."

    cd "$WORK_DIR"

    # Test to skip
    # TODO: enable these when we have a stable version of llama-stack client and server versions are aligned
    RC2_SKIP_TESTS=" or test_openai_completion_logprobs or test_openai_completion_logprobs_streaming or test_openai_chat_completion_structured_output or test_multiple_tools_with_different_schemas or test_mcp_tools_in_inference or test_tool_with_complex_schema or test_tool_without_schema"
    # TODO: re-enable the 2 chat_completion_non_streaming tests once they contain include max tokens (to prevent them from rambling)
    SKIP_TESTS="test_text_chat_completion_tool_calling_tools_not_in_request or test_text_chat_completion_structured_output or test_text_chat_completion_non_streaming or test_openai_chat_completion_non_streaming or test_openai_chat_completion_with_tool_choice_none or test_openai_chat_completion_with_tools or test_openai_format_preserves_complex_schemas $RC2_SKIP_TESTS"

    # Dynamically determine the path to config.yaml from the original script directory
    STACK_CONFIG_PATH="$SCRIPT_DIR/../distribution/config.yaml"
    if [ ! -f "$STACK_CONFIG_PATH" ]; then
        echo "Error: Could not find stack config at $STACK_CONFIG_PATH"
        exit 1
    fi

    uv venv
    # shellcheck source=/dev/null
    source .venv/bin/activate
    uv pip install llama-stack-client

    # Create conftest.py to patch llama-stack-client timeout for ARM64/QEMU emulation
    # The default httpx timeout is 5s which is way too short for emulated environments
    cat > tests/integration/inference/conftest_timeout.py << 'EOF'
import os
import pytest
from llama_stack_client import LlamaStackClient

# Store the original __init__ method
_original_init = LlamaStackClient.__init__

def patched_init(self, *args, **kwargs):
    # Set a much longer timeout for ARM64/QEMU emulation (default is 5 seconds)
    # We use 600 seconds (10 minutes) to accommodate very slow embedding operations
    if 'timeout' not in kwargs:
        kwargs['timeout'] = 600.0
    return _original_init(self, *args, **kwargs)

# Monkey patch the LlamaStackClient to use extended timeout
LlamaStackClient.__init__ = patched_init
EOF

    # Import the timeout patch before running tests
    export PYTHONPATH="$WORK_DIR/tests/integration/inference:${PYTHONPATH:-}"

    uv run pytest -s -v tests/integration/inference/ \
        --stack-config=server:"$STACK_CONFIG_PATH" \
        --text-model="$model" \
        --embedding-model="$EMBEDDING_MODEL" \
        -p conftest_timeout \
        -k "not ($SKIP_TESTS)"
}

function main() {
    echo "Starting llama-stack integration tests"
    echo "Configuration:"
    echo "  LLAMA_STACK_VERSION: $LLAMA_STACK_VERSION"
    echo "  LLAMA_STACK_REPO: $LLAMA_STACK_REPO"
    echo "  WORK_DIR: $WORK_DIR"
    echo "  VLLM_INFERENCE_MODEL: $VLLM_INFERENCE_MODEL"
    echo "  VERTEX_AI_INFERENCE_MODEL: $VERTEX_AI_INFERENCE_MODEL"
    echo "  OPENAI_INFERENCE_MODEL: $OPENAI_INFERENCE_MODEL"
    echo "  EMBEDDING_MODEL: $EMBEDDING_MODEL"
    echo "  VERTEX_AI_PROJECT: ${VERTEX_AI_PROJECT:-<not set>}"
    echo "  OPENAI_API_KEY: ${OPENAI_API_KEY:+<set>}"

    clone_llama_stack

    # Build list of models to test based on available configuration
    models_to_test=("$VLLM_INFERENCE_MODEL")

    # Only include Vertex AI models if VERTEX_AI_PROJECT is set
    if [ -n "${VERTEX_AI_PROJECT:-}" ]; then
        echo "VERTEX_AI_PROJECT is set, including Vertex AI models in tests"
        models_to_test+=("$VERTEX_AI_INFERENCE_MODEL")
    else
        echo "VERTEX_AI_PROJECT is not set, skipping Vertex AI models"
    fi

    # Only include OpenAI models if OPENAI_API_KEY is set
    if [ -n "${OPENAI_API_KEY:-}" ]; then
        echo "OPENAI_API_KEY is set, including OpenAI models in tests"
        models_to_test+=("$OPENAI_INFERENCE_MODEL")
    else
        echo "OPENAI_API_KEY is not set, skipping OpenAI models"
    fi

    for model in "${models_to_test[@]}"; do
        run_integration_tests "$model"
    done
    echo "Integration tests completed successfully!"
}


main "$@"
exit 0
